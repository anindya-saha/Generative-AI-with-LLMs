Use an end-to-end model hosting solution to host code llama. Fast API for custom preprocessing and post processing + HF TGI (Text Generation Inference) server to host the model + Capture FAST API logs, HF TGI logs through Fluentd to ElasticSearch and visualize in Grafana, Capture Fast API metrics and TGI metrics in Prometheus + Grafana to visualize, Load test using Locust. - Everything locally via Docker compose.