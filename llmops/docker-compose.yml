version: '3.8'

services:
  fastapi:
    build: ./fastapi
    ports:
      - "8000:8000"
    depends_on:
      - fluentd
    logging:
      driver: fluentd
      options:
        fluentd-address: localhost:24224
        tag: fastapi.access

  tgi:
    image: ghcr.io/huggingface/text-generation-inference:1.3.3
    command: ${INFERENCE_COMMAND}
    shm_size: 1g
    env_file:
      - .env
    ports:
      - "8080:80"
    volumes:
      - ${VOLUME}:/data
    depends_on:
      - fluentd
    logging:
      driver: fluentd
      options:
        fluentd-address: localhost:24224
        tag: tgi.access
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  fluentd:
    build: ./fluentd
    volumes:
      - ./fluentd/conf:/fluentd/etc
    ports:
      - "24224:24224"
      - "24224:24224/udp"

  elasticsearch:
    image: elasticsearch:7.13.1
    environment:
      - discovery.type=single-node
    ports:
      - "9200:9200"

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    depends_on:
      - elasticsearch

  prometheus:
    image: prom/prometheus
    volumes:
      - ./prometheus:/etc/prometheus
    ports:
      - "9090:9090"
    depends_on:
      - fastapi
      - tgi

  locust:
    image: locustio/locust
    volumes:
      - ./locust:/mnt/locust
    ports:
      - "8089:8089"
    command: -f /mnt/locust/locustfile.py --host=http://fastapi:8000

