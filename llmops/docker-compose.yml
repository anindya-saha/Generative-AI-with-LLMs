version: '3.8'

services:

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.13.1
    platform: linux/amd64
    environment:
      - discovery.type=single-node
      - network.host=0.0.0.0
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms128m -Xmx512m"
    volumes:
      - esdata1:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
    expose:
      - "9200"

  fluentd:
    build: ./fluentd
    volumes:
      - ./fluentd/conf:/fluentd/etc
    depends_on:
      - elasticsearch
    ports:
      - "24224:24224"
      - "24224:24224/udp"
      
  kibana:
    image: docker.elastic.co/kibana/kibana:7.13.1
    platform: linux/amd64
    environment:
      ELASTICSEARCH_URL: "http://elasticsearch:9200"
    depends_on:
      - elasticsearch
    ports:
      - "5601:5601"
  
  tgi:
    image: ghcr.io/huggingface/text-generation-inference:2.0.2
    platform: linux/amd64
    command: ["--model-id", "HuggingFaceH4/zephyr-7b-beta"]
    shm_size: 1g
    env_file:
      - .env
    ports:
      - "8080:80"
    volumes:
      - ./tgi/data:/data
    depends_on:
      - fluentd
    logging:
      driver: fluentd
      options:
        fluentd-address: localhost:24224
        tag: tgi.service
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  fastapi:
    build: ./fastapi
    platform: linux/amd64
    environment:
      - HF_API_URL=http://tgi:80  # Use this if FastAPI and TGI are in the same Docker network
      - OTEL_EXPORTER_JAEGER_ENDPOINT=http://jaeger:14268/api/traces  # Jaeger endpoint
      - OTEL_RESOURCE_ATTRIBUTES=service.name=inference_api  # Service name
    ports:
      - "8000:8000"
    depends_on:
      - tgi
      - fluentd
    logging:
      driver: fluentd
      options:
        fluentd-address: localhost:24224
        tag: fastapi.service


  prometheus:
    image: prom/prometheus:v2.52.0
    platform: linux/amd64
    volumes:
      - ./prometheus:/etc/prometheus
    ports:
      - "9090:9090"
    depends_on:
      - tgi
      - fastapi


  grafana:
    image: grafana/grafana:8.1.2
    platform: linux/amd64
    ports:
      - "3000:3000"
    volumes:
      - ./grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
      - ./grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/provisioning/grafana.ini:/etc/grafana/grafana.ini
      - ./grafana/dashboards:/var/lib/grafana/dashboards
    depends_on:
      - prometheus

  jaeger:
    image: jaegertracing/all-in-one:latest
    platform: linux/amd64
    ports:
      - "14268:14268"    # Jaeger Service port
      - "16686:16686"    # Jaeger UI port
      - "6831:6831/udp"  # Jaeger Thrift compact protocol
      - "6832:6832/udp"  # Jaeger Thrift binary protocol
  
  # locust:
  #   image: locustio/locust
  #   volumes:
  #     - ./locust:/mnt/locust
  #   ports:
  #     - "8089:8089"
  #   command: -f /mnt/locust/locustfile.py --host=http://fastapi:8000

networks:
  monitoring:

volumes:
  esdata1:
    driver: local