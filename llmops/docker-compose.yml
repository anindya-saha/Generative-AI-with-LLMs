version: '3.8'

services:

  fastapi:
    build: ./fastapi
    environment:
      - HF_API_URL=http://tgi:80  # Use this if FastAPI and TGI are in the same Docker network
    ports:
      - "8000:8000"
    depends_on:
      - tgi
      - fluentd
    logging:
      driver: fluentd
      options:
        fluentd-address: localhost:24224
        tag: fastapi.service
  
  tgi:
    image: ghcr.io/huggingface/text-generation-inference:1.4
    command: ["--model-id", "HuggingFaceH4/zephyr-7b-beta"]
    shm_size: 1g
    env_file:
      - .env
    ports:
      - "8080:80"
    volumes:
      - ./tgi/data:/data
    depends_on:
      - fluentd
    logging:
      driver: fluentd
      options:
        fluentd-address: localhost:24224
        tag: tgi.service
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  fluentd:
    build: ./fluentd
    volumes:
      - ./fluentd/conf:/fluentd/etc
    depends_on:
      - elasticsearch
    ports:
      - "24224:24224"
      - "24224:24224/udp"

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.13.1
    environment:
      - discovery.type=single-node
      - network.host=0.0.0.0
    ports:
      - "9200:9200"
    expose:
      - "9200"

  kibana:
    image: docker.elastic.co/kibana/kibana:7.13.1
    depends_on:
      - elasticsearch
    ports:
      - "5601:5601"

  # grafana:
  #   image: ghcr.io/grafana/grafana
  #   ports:
  #     - "3000:3000"
  #   depends_on:
  #     - elasticsearch

  # prometheus:
  #   image: ghcr.io/prom/prometheus
  #   volumes:
  #     - ./prometheus:/etc/prometheus
  #   ports:
  #     - "9090:9090"
  #   depends_on:
  #     - fastapi
  #     - tgi

  # locust:
  #   image: locustio/locust
  #   volumes:
  #     - ./locust:/mnt/locust
  #   ports:
  #     - "8089:8089"
  #   command: -f /mnt/locust/locustfile.py --host=http://fastapi:8000

