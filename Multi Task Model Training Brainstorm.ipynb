{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b5e0706-0a32-4958-93b4-178549bfc24b",
   "metadata": {},
   "source": [
    "## PEFT Model Merging - Multi Tasking from the same Base Model with specific adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a9fcdf6-e1e7-4644-a88d-a8a66bf6ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ee2c201-56af-49e9-89d1-29798682c07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anindya/deepseek-tune/lib/python3.8/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "from peft import PeftConfig, PeftModel\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b8c307-26de-4d04-8ca3-d6908067b2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = \"smangrul/tinyllama_lora_norobots\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3c34ad3-5bb6-40b7-9869-4a711ced15f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PeftConfig.from_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c1b0b04-9c76-4bac-83de-c3f76475bc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.base_model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3bef130-989d-479b-b2a6-d81d222a4064",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = 'TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bd6b09b-0973-479e-8ebe-f57a19765361",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_path, load_in_4bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce456fe4-6480-4b90-92a9-d097122e8c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1faee043-348c-4305-bbdc-332c4803e4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43165885-5492-47b3-bb70-cce09ddab2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "/home/anindya/deepseek-tune/lib/python3.8/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s> [INST] Write an essay about Generative AI. [/INST]\n",
      "Write a paper on the following topic: \"The Future of Artificial Intelligence\" [FUTURE] Write a paper on the following topic: \"The Future of Artificial Intelligence\". [/FUTURE]\n",
      "[INTELLECTUAL PROPERTY] Write a paper on the following topic: \"Intellectual Property and its Impact on Society\" [IP] Write a paper on the following topic: \"Intellectual Property and its Impact on Society\". [/IP]\n",
      "[ECONOMICS] Write a paper on the following topic: \"How to Make Money in Economics?\" [ECON] Write a paper on the following topic: \"How to Make Money in Economics?\". [/ECON]\n",
      "[MATHEMATICS] Write a paper on the following topic: \"Mathematical Problems for Students\" [MATH] Write a paper on the following topic: \"Mathematical Problems for Students\". [/MATH]\n",
      "[PHYSICS] Write a paper on the following topic: \"Physics Problems\" [PHY] Write a paper on the following topic: \"Physics Problems\".\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Write an essay about Generative AI.\"},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")  # , add_special_tokens=False)\n",
    "inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "outputs = base_model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.2,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8fa5bb3-249d-41df-9946-6ddbf2b027fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s> [INST] <<SYS>>\n",
      "Create a text ad given the following product and description.\n",
      "<</SYS>>\n",
      "\n",
      "Product: Sony PS5 PlayStation Console\n",
      "Description: The PS5™ console unleashes new gaming possibilities that you never anticipated. [/INST]\n",
      "\n",
      "The above example is from an actual email I received from Microsoft, but it's pretty much what I was looking for in terms of formatting. \n",
      "I tried to use the same format as this one (https://www.microsoft.com/en-us/p/sonyps5console/9nblggh14062) but it didn't work out so well.\n",
      "Any help would be appreciated!\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Create a text ad given the following product and description.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Product: Sony PS5 PlayStation Console\\nDescription: The PS5™ console unleashes new gaming possibilities that you never anticipated.\",\n",
    "    },\n",
    "]\n",
    "text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")  # , add_special_tokens=False)\n",
    "inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "outputs = base_model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.2,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9f9a049-ccdf-40b5-a7f7-b329f5ff5f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Table: 2-11365528-2\n",
      "Columns: ['Team', 'Head Coach', 'President', 'Home Ground', 'Location']\n",
      "Natural Query: Who is the Head Coach of the team whose President is Mario Volarevic?\n",
      "SQL Query: SELECT Team, Head Coach, President, Home Ground, Location FROM Teams WHERE Head Coach = Mario Volarevic\n",
      "\n",
      "A: You can use a join to get all the columns you need.\n",
      "SELECT t.Team, t.HeadCoach, t.President, t.HomeGround\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Table: 2-11365528-2\n",
    "Columns: ['Team', 'Head Coach', 'President', 'Home Ground', 'Location']\n",
    "Natural Query: Who is the Head Coach of the team whose President is Mario Volarevic?\n",
    "SQL Query:\"\"\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")  # , add_special_tokens=False)\n",
    "inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "outputs = base_model.generate(\n",
    "    **inputs, max_new_tokens=64, repetition_penalty=1.1, eos_token_id=tokenizer(\"</s>\").input_ids[-1]\n",
    ")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42c02f4-3f0a-4999-86d2-f877745fa949",
   "metadata": {},
   "source": [
    "### Merge Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a6288eb-4dee-4a10-94e5-9d54a5ca5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = PeftConfig.from_pretrained(robots_peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46b9bd69-80a7-4ccc-9906-0bbb3a194535",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = 'TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cae0e72-abc1-4079-bd07-2c6ae80e09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_path, load_in_4bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "157259c4-454d-4ceb-856a-d1d8aa82bd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "robots_peft_model_id = \"smangrul/tinyllama_lora_norobots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "363e25c3-7892-45cc-8780-6f987743ecbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(robots_peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59dec9f7-56bc-4e7e-a713-9429750246e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32005, 2048)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5299787c-6af3-4851-a6cd-f9d710859109",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_peft_model = PeftModel.from_pretrained(base_model, robots_peft_model_id, adapter_name=\"norobots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a3ce21c-a8f2-41c5-924f-78db4d8230e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_peft_model_id = \"smangrul/tinyllama_lora_sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "779ccae7-b72c-4433-a4df-bcfcedf35c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adcopy_peft_model_id = \"smangrul/tinyllama_lora_adcopy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce0be6-fb31-48dd-ab09-dd221f5b5dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c32abf85-d2d6-4db5-9b6d-74290de723ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = combined_peft_model.load_adapter(sql_peft_model_id, adapter_name=\"sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee4f49c4-b797-4db4-81b7-a32c90dbcf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = combined_peft_model.load_adapter(adcopy_peft_model_id, adapter_name=\"adcopy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a917c9-107a-4347-a20c-15fdbcd3c7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f3c0b66-5795-4516-991e-f61446640666",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapters = [\"norobots\", \"adcopy\", \"sql\"]\n",
    "weights = [2.0, 0.3, 0.7]\n",
    "adapter_name = \"merge\"\n",
    "density = 0.2\n",
    "combination_type = \"ties\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "075ba463-498f-4868-beca-94011194c801",
   "metadata": {},
   "outputs": [],
   "source": [
    "if adapter_name in combined_peft_model.peft_config:\n",
    "    print(f\"deleting adapter {adapter_name}\")\n",
    "    model.delete_adapter(adapter_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f0d5815-8cb0-4063-9945-82e854a23956",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_peft_model.add_weighted_adapter(adapters, weights, adapter_name, combination_type=combination_type, density=density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "042bb448-30bb-40b5-9593-d4384b2bb8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_peft_model.eval()\n",
    "combined_peft_model.set_adapter(\"merge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3d2d63-3d47-4cea-9b8a-48f7011d9fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e7cf55-a071-4d6d-8154-f1ed0312573b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d7512bf-42d5-481f-84af-1cd887290a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><|im_start|>user \n",
      "Write an essay about Generative AI.<|im_end|> \n",
      "<|im_start|>assistant \n",
      "Write a paper on the topic of your choice, but make sure it is relevant to the theme of this conference. \n",
      "\n",
      "### Submission Instructions\n",
      "1. Please submit your submission through [this form](https://docs.google.com/forms/d/e/1FAIpiYXF76wv5Z2Q_K4qJ9G0jD38zR-MHVpNuEOgLmWkUyPxTtSsBcAiCfhbQnQa5l1rYoJO1i0YJY) by Monday, September 2nd at 11:59pm EST. If you are unable to complete the form in time for the deadline, please email <EMAIL> with the subject \"Submission Extension\". You will be required to provide proof that you have submitted the extension request before we can accept your submission.\n",
      "\n",
      "2. The following instructions apply only if you are submitting a poster presentation. For oral presentations, see below.\n",
      "\n",
      "#### Poster Presentation Guidelines\n",
      "Posters must be double sided and should not exceed \n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Write an essay about Generative AI.\"},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")  # , add_special_tokens=False)\n",
    "inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "outputs = combined_peft_model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.2,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec7068e-39c1-4e74-bc3d-550d3df6513f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb58ee64-6540-4b04-97ae-9ca8968eba57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Table: 2-11365528-2\n",
      "Columns: ['Team', 'Head Coach', 'President', 'Home Ground', 'Location']\n",
      "Natural Query: Who is the Head Coach of the team whose President is Mario Volarevic?\n",
      "SQL Query: SELECT Team, Head Coach, President, Home Ground, Location FROM Teams WHERE Head Coach = Mario Volarevic\n",
      "\n",
      "A: You can use a join to get all the columns you need.\n",
      "SELECT t.Team, t.HeadCoach, t.President, t.HomeGround\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Table: 2-11365528-2\n",
    "Columns: ['Team', 'Head Coach', 'President', 'Home Ground', 'Location']\n",
    "Natural Query: Who is the Head Coach of the team whose President is Mario Volarevic?\n",
    "SQL Query:\"\"\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")  # , add_special_tokens=False)\n",
    "inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "outputs = combined_peft_model.generate(\n",
    "    **inputs, max_new_tokens=64, repetition_penalty=1.1, eos_token_id=tokenizer(\"</s>\").input_ids[-1]\n",
    ")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51c39df-e48e-4759-a16c-71aca9ed29a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek-tune",
   "language": "python",
   "name": "deepseek-tune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
